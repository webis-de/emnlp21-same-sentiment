{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# SameSentiment Yelp - Pair Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers.trainer_utils import set_seed\n",
    "\n",
    "tqdm.pandas()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# download + scp to server + extract\n",
    "data_yelp_path = Path(\"data/sentiment/yelp/\")\n",
    "\n",
    "# ------------------------------------\n",
    "\n",
    "# local?\n",
    "data_yelp_path = Path(\"data_raw/sentiment/yelp/\")\n",
    "\n",
    "# local? - output path (base) for sentiment review yelp pairs\n",
    "data_yelp_b_tdt_path = Path(\"data/sentiment/yelp-pair-b/\")\n",
    "data_yelp_b_rand_tdt_path = Path(\"data/sentiment/yelp-pair-rand-b/\")\n",
    "# local - output path for simple sentiment reviews yelp\n",
    "data_yelp_tdt_sentiment_5_path = Path(\"data/sentiment/yelp-sentiment-5/\")\n",
    "data_yelp_tdt_sentiment_b_path = Path(\"data/sentiment/yelp-sentiment-b/\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dn_yelp_cached = data_yelp_path / \"cached\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#  #### Load categories & topics\n",
    "from data_prep import load_reviews, load_topics\n",
    "\n",
    "# ##### Filter categories\n",
    "from data_prep import filter_min_cat_combis, make_map_cats, make_cat_combis\n",
    "\n",
    "# ##### Filter reviews\n",
    "from data_prep import filter_min_review_freq, filter_both_good_bad\n",
    "\n",
    "\n",
    "# #### Load category tree\n",
    "from data_prep import load_category_tree\n",
    "\n",
    "\n",
    "# #### Cache root category reviews in dataframes\n",
    "from data_prep import cache_root_category_businesses_df, load_cached_root_category_businesses_df\n",
    "\n",
    "\n",
    "# #### Dataframe for training etc.\n",
    "from data_prep import make_or_load_pairs\n",
    "\n",
    "\n",
    "# #### Make train/dev/test splits\n",
    "from data_prep import split_df, write_pair_df_tsv, write_pair_tdt_tsv\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load reviews"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "fn_yelp_reviews = data_yelp_path / \"review.json\"\n",
    "df = load_reviews(fn_yelp_reviews)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "6685900it [00:47, 140619.36it/s]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load categories for businesses\n",
    "\n",
    "- business (id) with list of topics/categories\n",
    "- lookups (business -> categories, category -> businesses)\n",
    "- list of combinations (with amount)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fn_yelp_topics = data_yelp_path / \"business.json\"\n",
    "bids_not_cats = set()\n",
    "inv_bid_cats = load_topics(fn_yelp_topics, bids_not_cats=bids_not_cats)\n",
    "\n",
    "inv_cat_bids = make_map_cats(inv_bid_cats)\n",
    "\n",
    "inv_cat_combis = make_cat_combis(inv_bid_cats)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load category tree\n",
    "\n",
    "- hierarchy of categories"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "fn_yelp_catgory_tree = data_yelp_path / \"all_category_list.json\"\n",
    "map_categories, map_cat_name2id, lst_root_categories = load_category_tree(fn_yelp_catgory_tree)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Pre-Cache all root category businesses (reviews)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "cache_root_category_businesses_df(df, inv_cat_bids, map_categories, map_cat_name2id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Make pairs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "if False:\n",
    "    set_seed(42)\n",
    "\n",
    "    fn_yelp_df = data_yelp_path / \"df_traindev4_typed.p\"\n",
    "    # delete if it exists, else it will only be loaded ...\n",
    "    if fn_yelp_df.exists():\n",
    "        print(f\"Remove prior dataframe: {fn_yelp_df}\")\n",
    "        fn_yelp_df.unlink()\n",
    "\n",
    "    df = filter_min_review_freq(df, min_ratings=8)\n",
    "    df = filter_both_good_bad(df)\n",
    "\n",
    "    traindev_df = make_or_load_pairs(df, inv_cat_bids, str(fn_yelp_df), num_pairs_per_class=4)\n",
    "\n",
    "    fn_yelp_df = data_yelp_path / \"df_traindev_test.p\"\n",
    "\n",
    "    # store\n",
    "    traindev_df, test_df = split_df(traindev_df, ratio=0.1, do_shuffle=True, random_state=42, name_train=\"traindev\", name_dev=\"test\")\n",
    "\n",
    "    with open(fn_yelp_df, \"wb\") as fp:\n",
    "        pickle.dump(traindev_df, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        pickle.dump(test_df, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "fn_yelp_df = data_yelp_path / \"df_traindev_test.p\"\n",
    "\n",
    "with open(fn_yelp_df, \"rb\") as fp:\n",
    "    traindev_df = pickle.load(fp)\n",
    "    test_df = pickle.load(fp)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "print(f\"num samples total: {len(test_df)}\")\n",
    "\n",
    "# check how many pairs are per pairing\n",
    "print(\"train/dev:\")\n",
    "for pairtype, df_grouped in traindev_df.groupby([\"type\"]):\n",
    "    print(f\"- {pairtype}: {len(df_grouped)}\")\n",
    "    # df_grouped.describe()\n",
    "\n",
    "print(\"test:\")\n",
    "for pairtype, df_grouped in test_df.groupby([\"type\"]):\n",
    "    print(f\"- {pairtype}: {len(df_grouped)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "num samples total: 70376\n",
      "train/dev:\n",
      "- bad-bad: 158255\n",
      "- bad-good: 158356\n",
      "- good-bad: 158236\n",
      "- good-good: 158537\n",
      "test:\n",
      "- bad-bad: 17685\n",
      "- bad-good: 17584\n",
      "- good-bad: 17704\n",
      "- good-good: 17403\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run test evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "#model_name = \"bert-base-cased\"\n",
    "#model_name = \"distilroberta-base\"\n",
    "\n",
    "data_name = \"yelp-pair-b\"\n",
    "#data_name = \"yelp-pair-rand-b\" ## over businesses\n",
    "\n",
    "seq_len = 256\n",
    "batch_size = 16\n",
    "acc_steps = 64\n",
    "num_epoch = 3\n",
    "cuda_devs = \"0\"\n",
    "\n",
    "run_name = f\"{model_name}-{data_name}_{seq_len}_{batch_size}-acc{acc_steps}_{num_epoch}\"\n",
    "\n",
    "#! mkdir ./output_sent/{run_name}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "for pairtype, df_grouped in test_df.groupby([\"type\"]):\n",
    "    print(f\"Eval {pairtype}: {len(df_grouped)}\")\n",
    "\n",
    "    fn_data_path = Path(f\"data/sentiment/{run_name}/{pairtype}\")\n",
    "    fn_data_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    write_pair_df_tsv(df_grouped, fn_data_path / \"test.tsv\", \"test\")\n",
    "    \n",
    "    ! CUDA_VISIBLE_DEVICES={cuda_devs} \\\n",
    "        python trainer.py \\\n",
    "        --do_test \\\n",
    "        --model_name_or_path ./output_sent/{run_name} \\\n",
    "        --task_name same-b \\\n",
    "        --data_dir {fn_data_path} \\\n",
    "        --output_dir ./output_sent/{run_name}/pairtype/{pairtype} \\\n",
    "        --run_name {run_name}-{pairtype} \\\n",
    "        --per_device_eval_batch_size {batch_size} \\\n",
    "        --max_seq_length {seq_len}\n",
    "\n",
    "    print()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test: 100%|██████████| 17685/17685 [00:00<00:00, 303879.04it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Eval bad-bad: 17685\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WARNING|trainer.py:194] 02/01/2021 22:07:48 >> Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "[INFO|trainer.py:204] 02/01/2021 22:07:48 >> Training/evaluation parameters MyTrainingArguments(output_dir='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pairtype/bad-bad', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_22-07-48_cuda2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='bert-base-uncased-yelp-pair-b_256_16-acc64_3-bad-bad', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, do_test=True)\n",
      "[INFO|trainer.py:205] 02/01/2021 22:07:48 >> Dataset parameters SamenessDataTrainingArguments(task_name='same-b', data_dir='data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-bad', max_seq_length=256, overwrite_cache=False, pad_to_max_length=True)\n",
      "[INFO|trainer.py:206] 02/01/2021 22:07:48 >> Model parameters ModelArguments(model_name_or_path='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True)\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:07:48,332 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:07:48,332 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:07:48,332 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:07:48,333 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2021-02-01 22:07:48,333 >> Model name './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:07:48,333 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:07:48,333 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:07:48,333 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:07:48,333 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:07:48,333 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:07:48,333 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:07:48,333 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:938] 2021-02-01 22:07:48,356 >> loading weights file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1056] 2021-02-01 22:07:50,367 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1064] 2021-02-01 22:07:50,367 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[INFO|filelock.py:274] 02/01/2021 22:07:50 >> Lock 140361929792624 acquired on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-bad/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "[INFO|datasets.py:162] 02/01/2021 22:07:50 >> Creating features from dataset file at data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-bad\n",
      "[INFO|processors.py:119] 02/01/2021 22:07:52 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:07:52 >> guid: test-177379\n",
      "[INFO|processors.py:121] 02/01/2021 22:07:52 >> features: InputFeatures(input_ids=[101, 11519, 2326, 1010, 11519, 2833, 1012, 2074, 1996, 2157, 3815, 1997, 2833, 1012, 2134, 1005, 1056, 2729, 2005, 1996, 5624, 14540, 10376, 2295, 1012, 1037, 2210, 20857, 1012, 102, 4299, 1045, 2018, 2242, 3893, 2000, 2360, 1012, 1996, 15812, 2001, 2061, 12726, 999, 2123, 1005, 1056, 5949, 2115, 2051, 1012, 2057, 2057, 2097, 2196, 2175, 2067, 1012, 1012, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:07:52 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:07:52 >> guid: test-659980\n",
      "[INFO|processors.py:121] 02/01/2021 22:07:52 >> features: InputFeatures(input_ids=[101, 5409, 3325, 1045, 1005, 2310, 2412, 2018, 1999, 2151, 4825, 2412, 1012, 2256, 2972, 2795, 1997, 1017, 2288, 5305, 3202, 2044, 5983, 2182, 1010, 2023, 2173, 3791, 2000, 2022, 2988, 2000, 1996, 2740, 2533, 1998, 2701, 2091, 1012, 102, 1045, 2507, 7842, 4246, 4948, 8983, 1037, 1017, 1015, 1013, 1016, 3340, 1010, 1996, 7224, 2003, 3835, 1998, 4010, 2021, 1037, 2978, 3976, 2100, 1012, 1996, 2326, 2003, 2200, 2658, 2021, 1996, 4664, 2003, 2235, 1012, 2057, 2031, 2177, 1997, 2274, 1998, 8439, 2028, 1005, 1055, 5798, 1012, 2057, 2344, 2176, 24792, 27333, 2050, 2021, 2027, 2507, 2149, 1996, 3587, 2028, 2005, 2489, 1012, 2057, 3745, 1996, 4632, 11589, 28005, 2121, 2005, 10439, 20624, 6290, 1012, 2057, 2066, 1996, 15415, 13433, 3683, 2850, 1998, 23438, 4328, 12849, 5092, 2497, 1996, 2190, 1012, 2057, 2344, 2048, 6583, 2319, 1024, 20548, 1998, 20949, 1010, 2119, 2024, 6581, 2005, 2035, 1996, 3082, 12901, 4596, 5127, 1024, 9092, 23835, 3816, 18651, 1010, 7975, 16137, 7911, 1010, 12559, 12849, 17830, 1998, 20130, 1004, 8040, 8095, 7361, 14855, 2140, 13258, 5831, 1012, 1996, 9092, 23835, 18651, 2003, 2978, 4318, 2021, 1996, 15415, 15478, 2234, 2007, 2009, 2003, 2200, 2204, 1012, 2057, 2066, 1996, 12901, 1999, 1996, 23621, 9841, 1998, 1996, 12559, 12849, 17830, 1012, 1996, 7975, 16137, 7911, 2003, 2074, 7929, 1012, 2027, 2507, 2149, 1037, 16027, 3256, 6949, 18064, 2007, 13541, 2005, 1996, 5798, 2611, 2003, 1037, 3835, 3543, 1012, 1996, 4596, 4664, 2003, 2978, 2235, 1998, 1996, 2833, 2003, 3492, 3976, 2100, 1012, 3452, 2009, 2003, 2028, 1997, 1996, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:07:52 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:07:52 >> guid: test-258991\n",
      "[INFO|processors.py:121] 02/01/2021 22:07:52 >> features: InputFeatures(input_ids=[101, 2025, 2204, 2012, 2035, 4902, 1012, 2023, 2173, 2109, 2000, 2022, 2307, 1012, 1996, 2980, 1998, 14768, 11350, 12595, 2069, 2066, 29387, 1012, 1996, 1055, 2480, 5403, 7507, 4609, 7975, 2001, 2069, 3684, 1010, 2053, 14894, 1012, 102, 1045, 2064, 1005, 1056, 2903, 2054, 1037, 3532, 3325, 2057, 2018, 2007, 4287, 5833, 2651, 1012, 1996, 2236, 24529, 2080, 1005, 1055, 7975, 2018, 2069, 1021, 2235, 4109, 1997, 7975, 1010, 2021, 6197, 1997, 2665, 11565, 1998, 20949, 1012, 1045, 1005, 2310, 2196, 2464, 2008, 1999, 24529, 2080, 1010, 1998, 1045, 2001, 9364, 1012, 1996, 12901, 2001, 1037, 14894, 3238, 2829, 24665, 11431, 2100, 1012, 2043, 2057, 2170, 1010, 1996, 2611, 2006, 1996, 3042, 2001, 12726, 1010, 14477, 18155, 23884, 4588, 1998, 2056, 2008, 2673, 3310, 17463, 8684, 18655, 1012, 2061, 1012, 1012, 1012, 1012, 2129, 2024, 2017, 1037, 2822, 4825, 2065, 2017, 2074, 2330, 2039, 8641, 1998, 3684, 2068, 1029, 1996, 4589, 7975, 2018, 2025, 2130, 1037, 9374, 1997, 4589, 1012, 2026, 2905, 2056, 1996, 4086, 1050, 14768, 2001, 3492, 11519, 1010, 2021, 3452, 2057, 2020, 3811, 9364, 1012, 2045, 2024, 7564, 1997, 2060, 2822, 3182, 2006, 1996, 2225, 2217, 2057, 2097, 2655, 2279, 2051, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:07:52 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:07:52 >> guid: test-56880\n",
      "[INFO|processors.py:121] 02/01/2021 22:07:52 >> features: InputFeatures(input_ids=[101, 2253, 2000, 8764, 12101, 1005, 1055, 2197, 2305, 2005, 4596, 1012, 2057, 2288, 2045, 2105, 19540, 1998, 2020, 8901, 2157, 2185, 2007, 2053, 11079, 1012, 3641, 1996, 19027, 12273, 5498, 2005, 2019, 10439, 20624, 6290, 1012, 2009, 2001, 10468, 1016, 21698, 6240, 18510, 2008, 2020, 2025, 7852, 2098, 2066, 1045, 2001, 3241, 2009, 2052, 2022, 1012, 1996, 12901, 2001, 2061, 17688, 2100, 2008, 2026, 4451, 2318, 2770, 2802, 1996, 2972, 7954, 2127, 2057, 2020, 2589, 1012, 2009, 2001, 2061, 10827, 2008, 2017, 2071, 2025, 5959, 2151, 1997, 1996, 2060, 26389, 1012, 2036, 15640, 2001, 1996, 15544, 6499, 9284, 1999, 1996, 19027, 12273, 5498, 2001, 24514, 2100, 1998, 8982, 2100, 1012, 2044, 2008, 2057, 4741, 2205, 2146, 2005, 2256, 12278, 2069, 2000, 2156, 1016, 2060, 7251, 2008, 2234, 1999, 2044, 2149, 4374, 2037, 12278, 2096, 2057, 4741, 1012, 2498, 20868, 17728, 4570, 2033, 2062, 2084, 2023, 1012, 2045, 1005, 1055, 2074, 2053, 8016, 2004, 2027, 2036, 3641, 24857, 10447, 2066, 9731, 1012, 1045, 3641, 1996, 26666, 2007, 6240, 18510, 1006, 1002, 2459, 2053, 16521, 2443, 1007, 1998, 2026, 3129, 2288, 1996, 8808, 16806, 10893, 1006, 1002, 2403, 1007, 1012, 1996, 27130, 2020, 4257, 1998, 2898, 2066, 10768, 4779, 16835, 2638, 2029, 1045, 2123, 1005, 1056, 2729, 2005, 2007, 2026, 26666, 1012, 2009, 2001, 2074, 7929, 1012, 2026, 19089, 16806, 10893, 2001, 2428, 2204, 2061, 2057, 2119, 102, 2204, 2833, 1012, 2070, 1997, 1996, 5167, 2024, 1037, 2718, 2030, 1037, 3335, 1012, 2021, 2009, 2064, 2022, 1037, 2978, 2058, 1011, 21125, 4102, 2000, 1996, 3737, 1012, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:07:52 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:07:52 >> guid: test-595609\n",
      "[INFO|processors.py:121] 02/01/2021 22:07:52 >> features: InputFeatures(input_ids=[101, 2023, 10834, 1054, 2149, 2003, 2061, 27776, 16998, 3550, 1012, 1045, 2079, 1056, 2066, 1996, 2126, 1996, 3573, 2003, 2275, 2039, 1998, 2673, 2003, 1037, 6752, 1012, 2057, 2234, 1999, 2005, 1037, 3321, 3170, 2102, 2138, 2026, 10834, 1054, 2149, 2134, 1005, 1056, 2031, 2009, 1999, 4518, 1012, 2673, 1999, 1996, 3573, 2001, 1037, 6752, 1998, 2027, 2790, 2000, 2022, 2460, 21121, 1012, 2673, 2001, 2074, 1999, 4487, 10286, 9447, 1012, 1045, 2876, 1005, 1056, 2272, 2067, 2000, 2023, 3573, 4983, 2057, 2018, 2000, 1012, 102, 1996, 5126, 3768, 3716, 1997, 1996, 2087, 3937, 5167, 1998, 2215, 2000, 2391, 2041, 2000, 1037, 6875, 2450, 1996, 1018, 2367, 8413, 1997, 1996, 3573, 2008, 2056, 8875, 2071, 4298, 2022, 1999, 1010, 2129, 2055, 4851, 1010, 4214, 1010, 2559, 2009, 2039, 1998, 3788, 1996, 8013, 2000, 1996, 8875, 1029, 1045, 2180, 1005, 1056, 2022, 2067, 1998, 2097, 2224, 2151, 5592, 5329, 1045, 1005, 2310, 2363, 3784, 1998, 3649, 1045, 2342, 1999, 1037, 18392, 1045, 1005, 2222, 2344, 2006, 9733, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "[INFO|datasets.py:196] 02/01/2021 22:07:55 >> Saving features into cached file data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-bad/cached_test_BertTokenizerFast_256_same-b [took 2.184 s]\n",
      "[INFO|filelock.py:318] 02/01/2021 22:07:55 >> Lock 140361929792624 released on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-bad/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "No 'DISCORD_TOKEN' or 'DISCORD_CHANNEL' set! Disable Discord notifier.\n",
      "[INFO|trainer.py:361] 02/01/2021 22:07:56 >> *** Evaluate 'test' ***\n",
      "[INFO|trainer.py:1333] 2021-02-01 22:07:56,580 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1334] 2021-02-01 22:07:56,580 >>   Num examples = 17685\n",
      "[INFO|trainer.py:1335] 2021-02-01 22:07:56,580 >>   Batch size = 16\n",
      "100%|██████████████████████████████████████▉| 1105/1106 [01:39<00:00, 10.98it/s]/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:4196: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|integrations.py:513] 2021-02-01 22:09:37,033 >> Trainer is attempting to log a value of \"{'not same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'same': {'precision': 1.0, 'recall': 0.8498162284421826, 'f1-score': 0.9188115180045241, 'support': 17685}, 'accuracy': 0.8498162284421826, 'macro avg': {'precision': 0.5, 'recall': 0.4249081142210913, 'f1-score': 0.45940575900226205, 'support': 17685}, 'weighted avg': {'precision': 1.0, 'recall': 0.8498162284421826, 'f1-score': 0.9188115180045241, 'support': 17685}}\" of type <class 'dict'> for key \"eval_class_report\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "[WARNING|integrations.py:301] 2021-02-01 22:09:37,035 >> Trainer is attempting to log a value of \"{'not same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'same': {'precision': 1.0, 'recall': 0.8498162284421826, 'f1-score': 0.9188115180045241, 'support': 17685}, 'accuracy': 0.8498162284421826, 'macro avg': {'precision': 0.5, 'recall': 0.4249081142210913, 'f1-score': 0.45940575900226205, 'support': 17685}, 'weighted avg': {'precision': 1.0, 'recall': 0.8498162284421826, 'f1-score': 0.9188115180045241, 'support': 17685}}\" of type <class 'dict'> for key \"eval/class_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|███████████████████████████████████████| 1106/1106 [01:40<00:00, 11.03it/s]\n",
      "[INFO|trainer.py:393] 02/01/2021 22:09:37 >> ***** Eval 'test' results same-b *****\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_loss = 0.3424665629863739\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_acc = 0.8498162284421826\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_f1 = 0.9188115180045241\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_acc_and_f1 = 0.8843138732233533\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_pearson = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_spearmanr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_corr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:09:37 >>   eval_class_report = {'accuracy': 0.8498162284421826,\n",
      " 'macro avg': {'f1-score': 0.45940575900226205,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.4249081142210913,\n",
      "               'support': 17685},\n",
      " 'not same': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
      " 'same': {'f1-score': 0.9188115180045241,\n",
      "          'precision': 1.0,\n",
      "          'recall': 0.8498162284421826,\n",
      "          'support': 17685},\n",
      " 'weighted avg': {'f1-score': 0.9188115180045241,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 0.8498162284421826,\n",
      "                  'support': 17685}}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test:   0%|          | 0/17584 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Eval bad-good: 17584\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test: 100%|██████████| 17584/17584 [00:00<00:00, 172089.27it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WARNING|trainer.py:194] 02/01/2021 22:09:38 >> Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "[INFO|trainer.py:204] 02/01/2021 22:09:38 >> Training/evaluation parameters MyTrainingArguments(output_dir='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pairtype/bad-good', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_22-09-38_cuda2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='bert-base-uncased-yelp-pair-b_256_16-acc64_3-bad-good', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, do_test=True)\n",
      "[INFO|trainer.py:205] 02/01/2021 22:09:38 >> Dataset parameters SamenessDataTrainingArguments(task_name='same-b', data_dir='data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-good', max_seq_length=256, overwrite_cache=False, pad_to_max_length=True)\n",
      "[INFO|trainer.py:206] 02/01/2021 22:09:38 >> Model parameters ModelArguments(model_name_or_path='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True)\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:09:38,897 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:09:38,897 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:09:38,897 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:09:38,897 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2021-02-01 22:09:38,897 >> Model name './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:09:38,897 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:09:38,897 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:09:38,897 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:09:38,897 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:09:38,897 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:09:38,897 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:09:38,897 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:938] 2021-02-01 22:09:38,920 >> loading weights file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1056] 2021-02-01 22:09:40,892 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1064] 2021-02-01 22:09:40,892 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[INFO|filelock.py:274] 02/01/2021 22:09:40 >> Lock 140441643248848 acquired on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-good/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "[INFO|datasets.py:162] 02/01/2021 22:09:40 >> Creating features from dataset file at data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-good\n",
      "[INFO|processors.py:119] 02/01/2021 22:09:42 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:09:42 >> guid: test-389904\n",
      "[INFO|processors.py:121] 02/01/2021 22:09:42 >> features: InputFeatures(input_ids=[101, 2440, 19380, 1010, 1045, 2234, 2182, 2012, 1021, 1024, 5187, 9737, 1998, 2027, 2485, 2012, 1022, 1024, 27995, 1010, 2061, 1045, 2001, 3407, 2027, 2020, 2583, 2000, 2191, 2033, 1037, 15890, 1012, 1045, 2001, 2559, 2830, 2000, 2023, 2173, 2005, 1037, 2146, 2051, 2044, 3752, 2107, 2307, 4391, 2013, 3507, 6300, 14277, 2545, 1998, 2428, 1012, 1012, 2040, 2071, 2360, 2053, 2000, 1037, 2204, 15890, 1029, 2288, 1996, 3539, 2051, 15890, 25025, 2000, 2175, 1011, 1996, 15890, 2993, 2001, 1037, 17798, 1010, 20482, 8808, 1010, 11611, 1010, 1998, 1037, 12090, 21122, 1012, 1012, 1012, 2021, 2008, 2001, 2009, 1010, 2053, 2292, 8525, 3401, 1010, 24444, 1010, 2030, 12851, 1010, 14415, 1010, 2030, 17710, 10649, 6279, 1012, 1045, 1005, 1049, 2025, 2469, 2065, 1045, 3641, 2009, 3308, 2030, 2065, 2009, 1005, 1055, 2008, 2035, 2009, 3310, 2007, 1029, 4641, 2039, 1010, 1996, 25025, 2003, 2074, 2007, 22201, 1010, 2025, 22201, 1009, 3769, 1012, 2001, 2428, 9364, 1010, 1998, 3246, 2008, 2009, 2071, 2022, 2074, 16833, 2098, 2039, 2000, 2746, 1999, 2397, 1012, 102, 10392, 4248, 3962, 2000, 6723, 2070, 15890, 2015, 1998, 2060, 3435, 1011, 2833, 24533, 1012, 1045, 2444, 2625, 2084, 1037, 3371, 3298, 2013, 1996, 4497, 1998, 1045, 1005, 1049, 1999, 2045, 2012, 2560, 1015, 2296, 1022, 3134, 1012, 18178, 3567, 4523, 1010, 12559, 15890, 1010, 13433, 21823, 2638, 1010, 18474, 15890, 1010, 7975, 1010, 16521, 2015, 2009, 1005, 1055, 2035, 2307, 3737, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:09:42 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:09:42 >> guid: test-582056\n",
      "[INFO|processors.py:121] 02/01/2021 22:09:42 >> features: InputFeatures(input_ids=[101, 2123, 16446, 2024, 4010, 1998, 5510, 2204, 1012, 3095, 2003, 13971, 1998, 5466, 2123, 16446, 2043, 2027, 2228, 2017, 4995, 1005, 1056, 3666, 1012, 18307, 2005, 1996, 4268, 1010, 2489, 2300, 26536, 1012, 24094, 2024, 2205, 2152, 1010, 1045, 2156, 3071, 1005, 1055, 4268, 3564, 2039, 2006, 2037, 5042, 1012, 2045, 1005, 1055, 1037, 7752, 2000, 9378, 2115, 2398, 2021, 2053, 5723, 2503, 1012, 2009, 1005, 1055, 2204, 2065, 2017, 2066, 4010, 2123, 16446, 1012, 2027, 2071, 13387, 1996, 8397, 2125, 2205, 102, 2165, 1996, 2155, 2182, 2005, 1996, 2034, 2051, 999, 1996, 4268, 2020, 2061, 7568, 2000, 7661, 4697, 2037, 2219, 2123, 4904, 1012, 1996, 12183, 2018, 2061, 2116, 2307, 7047, 1012, 1996, 2123, 16446, 2234, 1997, 4840, 1998, 4010, 1012, 2026, 2365, 2288, 5474, 2098, 19443, 2007, 1049, 1004, 1049, 1005, 1055, 1012, 2026, 2684, 2288, 16876, 2460, 17955, 1012, 2026, 3129, 2288, 7967, 3139, 9115, 1998, 1045, 2288, 5474, 2098, 19443, 1012, 2057, 2035, 5632, 2009, 2200, 2172, 1012, 2057, 2097, 2022, 2067, 2153, 2005, 2469, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:09:42 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:09:42 >> guid: test-430633\n",
      "[INFO|processors.py:121] 02/01/2021 22:09:42 >> features: InputFeatures(input_ids=[101, 2058, 18098, 6610, 2094, 1998, 2053, 4495, 2000, 2202, 5056, 1997, 4748, 4341, 2012, 2023, 26226, 2015, 2006, 1996, 6167, 1012, 8054, 2005, 2216, 1999, 2342, 1997, 27962, 1010, 6544, 1998, 21705, 1012, 2017, 2024, 2583, 2000, 2224, 19054, 14189, 1012, 102, 2044, 2383, 4596, 1010, 2057, 2081, 2256, 3328, 2067, 2000, 24373, 4710, 3016, 1012, 2021, 2023, 2001, 2025, 2077, 7458, 2011, 26226, 2015, 2005, 2070, 25408, 5167, 1012, 2023, 26226, 2015, 3295, 7719, 2247, 7136, 1038, 2140, 16872, 1010, 2061, 1996, 2173, 2003, 2471, 2467, 8966, 1012, 2023, 3295, 2003, 2502, 1010, 1998, 2038, 1037, 2843, 1997, 12612, 1010, 2061, 1045, 2467, 2424, 3649, 1045, 1005, 1049, 2559, 2005, 2182, 1012, 1045, 24802, 2039, 2006, 11848, 5134, 1998, 2060, 5167, 1010, 1998, 2081, 2026, 2126, 2000, 4638, 5833, 1012, 1999, 8741, 1997, 1037, 2146, 2240, 1010, 1996, 3524, 2000, 4638, 5833, 2001, 2025, 2008, 2146, 1012, 2023, 3573, 2003, 14057, 2135, 2284, 1010, 2092, 24802, 1010, 1998, 2092, 2448, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:09:42 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:09:42 >> guid: test-462739\n",
      "[INFO|processors.py:121] 02/01/2021 22:09:42 >> features: InputFeatures(input_ids=[101, 2023, 2003, 1037, 9467, 2000, 2424, 2041, 2008, 1996, 4157, 14068, 3594, 9898, 2015, 1999, 2037, 9686, 20110, 2891, 1012, 2122, 9898, 2015, 5383, 1000, 10500, 1000, 2029, 2965, 2009, 2003, 2025, 1043, 7630, 6528, 2489, 1012, 1012, 2027, 2409, 2033, 2027, 2018, 2498, 2045, 2008, 2001, 1043, 7630, 6528, 2489, 1012, 2008, 1005, 1055, 2025, 2204, 1012, 10047, 2589, 2007, 1996, 4157, 14068, 1012, 2008, 1005, 1055, 2205, 2919, 1010, 2027, 2342, 2000, 2131, 1999, 1996, 2085, 1010, 2066, 2116, 2500, 2008, 8752, 2111, 2008, 2031, 2740, 9259, 1012, 1045, 2079, 2174, 2066, 2000, 5254, 2008, 2027, 2079, 3749, 1037, 2512, 1011, 11825, 6949, 2121, 1010, 8038, 2100, 1010, 2049, 1037, 2707, 2005, 2026, 2060, 3277, 1012, 2049, 2025, 2074, 2033, 1012, 1012, 1012, 7564, 1997, 2111, 2041, 2045, 2031, 2035, 7957, 1997, 2035, 2121, 17252, 1998, 2024, 4975, 2740, 3314, 1010, 2632, 4140, 1997, 2029, 2003, 3426, 2011, 1996, 2126, 102, 8884, 29500, 8013, 2182, 1011, 2667, 2041, 2023, 3962, 2144, 2009, 1005, 1055, 2625, 2084, 1037, 3542, 2013, 2026, 2188, 1012, 2651, 1045, 2018, 1037, 2312, 28248, 4157, 1011, 2029, 1045, 2001, 5191, 2055, 2009, 2108, 8618, 1012, 1012, 1012, 2021, 2009, 2003, 1037, 3835, 7782, 28248, 4157, 2025, 2058, 1011, 28115, 2066, 29500, 999, 999, 4606, 5717, 10250, 18909, 1998, 2053, 5699, 1045, 2064, 2079, 2023, 999, 1996, 5356, 3771, 2001, 10124, 1999, 8290, 1011, 2295, 2025, 12726, 1012, 2026, 2767, 2288, 2242, 2007, 26011, 6501, 1004, 16027, 9805, 2213, 1011, 2667, 2008, 2279, 2051, 1012, 2066, 2672, 4826, 1012, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:09:42 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:09:42 >> guid: test-559780\n",
      "[INFO|processors.py:121] 02/01/2021 22:09:42 >> features: InputFeatures(input_ids=[101, 2057, 2699, 3748, 24799, 1005, 1055, 3728, 1012, 1045, 2064, 1005, 1056, 2360, 2008, 1045, 2052, 2175, 2067, 2004, 1996, 27263, 2229, 2024, 2152, 1010, 1996, 10514, 6182, 2003, 2074, 7929, 1010, 1998, 1996, 26445, 11283, 5897, 1998, 2326, 2024, 4942, 11968, 1012, 7777, 1024, 1996, 2665, 6763, 1998, 10654, 4897, 2018, 1037, 3835, 6805, 2000, 2009, 1998, 1045, 4669, 2009, 18959, 2015, 1024, 1996, 10514, 6182, 1012, 1996, 9804, 2024, 2200, 3132, 1010, 1996, 8810, 2024, 2200, 2235, 2005, 1996, 3976, 1010, 1998, 1996, 8312, 2003, 3532, 1012, 1996, 6804, 4897, 2001, 2025, 2004, 2204, 2004, 2500, 1999, 1996, 2181, 1010, 2205, 3514, 2100, 2005, 2026, 5510, 1012, 21705, 1024, 2348, 2009, 2038, 1996, 2773, 8633, 1999, 2009, 1005, 1055, 2171, 2123, 1005, 1056, 5987, 2205, 2172, 1012, 2027, 4287, 2069, 2028, 1000, 2887, 1000, 5404, 29465, 2029, 2003, 2081, 2011, 4679, 1999, 1996, 2149, 1012, 2045, 2003, 2053, 12183, 2005, 8739, 2061, 2017, 2031, 2000, 2074, 2344, 2009, 2007, 2053, 2801, 2006, 2054, 2009, 2003, 2030, 2054, 2009, 2097, 3465, 1012, 2130, 1996, 2512, 1011, 14813, 9804, 2024, 3532, 1012, 2065, 2017, 2344, 5572, 1006, 2788, 3492, 2204, 1999, 2087, 10514, 6182, 7884, 1007, 2017, 1005, 102, 10392, 10514, 6182, 1998, 1037, 2200, 8363, 4044, 1012, 2023, 2003, 1037, 2442, 3046, 5023, 17070, 1012, 1045, 3641, 2195, 4395, 1998, 2296, 2309, 2028, 2790, 2000, 5510, 2488, 2084, 1996, 3025, 1012, 1996, 3095, 2003, 2200, 5379, 1998, 2012, 6528, 6024, 1012, 1045, 2156, 2870, 2437, 2023, 1037, 3180, 3962, 1997, 3067, 1024, 1007, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|datasets.py:196] 02/01/2021 22:09:45 >> Saving features into cached file data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-good/cached_test_BertTokenizerFast_256_same-b [took 2.162 s]\n",
      "[INFO|filelock.py:318] 02/01/2021 22:09:45 >> Lock 140441643248848 released on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/bad-good/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "No 'DISCORD_TOKEN' or 'DISCORD_CHANNEL' set! Disable Discord notifier.\n",
      "[INFO|trainer.py:361] 02/01/2021 22:09:46 >> *** Evaluate 'test' ***\n",
      "[INFO|trainer.py:1333] 2021-02-01 22:09:46,682 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1334] 2021-02-01 22:09:46,682 >>   Num examples = 17584\n",
      "[INFO|trainer.py:1335] 2021-02-01 22:09:46,682 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████| 1099/1099 [01:40<00:00, 10.93it/s]/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:4196: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|integrations.py:513] 2021-02-01 22:11:27,380 >> Trainer is attempting to log a value of \"{'not same': {'precision': 1.0, 'recall': 0.852195177434031, 'f1-score': 0.9202001903650711, 'support': 17584}, 'same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'accuracy': 0.852195177434031, 'macro avg': {'precision': 0.5, 'recall': 0.4260975887170155, 'f1-score': 0.46010009518253553, 'support': 17584}, 'weighted avg': {'precision': 1.0, 'recall': 0.852195177434031, 'f1-score': 0.9202001903650711, 'support': 17584}}\" of type <class 'dict'> for key \"eval_class_report\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "[WARNING|integrations.py:301] 2021-02-01 22:11:27,382 >> Trainer is attempting to log a value of \"{'not same': {'precision': 1.0, 'recall': 0.852195177434031, 'f1-score': 0.9202001903650711, 'support': 17584}, 'same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'accuracy': 0.852195177434031, 'macro avg': {'precision': 0.5, 'recall': 0.4260975887170155, 'f1-score': 0.46010009518253553, 'support': 17584}, 'weighted avg': {'precision': 1.0, 'recall': 0.852195177434031, 'f1-score': 0.9202001903650711, 'support': 17584}}\" of type <class 'dict'> for key \"eval/class_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|███████████████████████████████████████| 1099/1099 [01:40<00:00, 10.93it/s]\n",
      "[INFO|trainer.py:393] 02/01/2021 22:11:27 >> ***** Eval 'test' results same-b *****\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_loss = 0.32483938336372375\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_acc = 0.852195177434031\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_f1 = 0.0\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_acc_and_f1 = 0.4260975887170155\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_pearson = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_spearmanr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_corr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:11:27 >>   eval_class_report = {'accuracy': 0.852195177434031,\n",
      " 'macro avg': {'f1-score': 0.46010009518253553,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.4260975887170155,\n",
      "               'support': 17584},\n",
      " 'not same': {'f1-score': 0.9202001903650711,\n",
      "              'precision': 1.0,\n",
      "              'recall': 0.852195177434031,\n",
      "              'support': 17584},\n",
      " 'same': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
      " 'weighted avg': {'f1-score': 0.9202001903650711,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 0.852195177434031,\n",
      "                  'support': 17584}}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test:   0%|          | 0/17704 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Eval good-bad: 17704\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test: 100%|██████████| 17704/17704 [00:00<00:00, 163155.44it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WARNING|trainer.py:194] 02/01/2021 22:11:29 >> Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "[INFO|trainer.py:204] 02/01/2021 22:11:29 >> Training/evaluation parameters MyTrainingArguments(output_dir='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pairtype/good-bad', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_22-11-29_cuda2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='bert-base-uncased-yelp-pair-b_256_16-acc64_3-good-bad', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, do_test=True)\n",
      "[INFO|trainer.py:205] 02/01/2021 22:11:29 >> Dataset parameters SamenessDataTrainingArguments(task_name='same-b', data_dir='data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-bad', max_seq_length=256, overwrite_cache=False, pad_to_max_length=True)\n",
      "[INFO|trainer.py:206] 02/01/2021 22:11:29 >> Model parameters ModelArguments(model_name_or_path='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True)\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:11:29,261 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:11:29,262 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:11:29,262 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:11:29,262 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2021-02-01 22:11:29,262 >> Model name './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:11:29,262 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:11:29,262 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:11:29,262 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:11:29,262 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:11:29,262 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:11:29,262 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:11:29,262 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:938] 2021-02-01 22:11:29,287 >> loading weights file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1056] 2021-02-01 22:11:31,258 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1064] 2021-02-01 22:11:31,258 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[INFO|filelock.py:274] 02/01/2021 22:11:31 >> Lock 139997468515536 acquired on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-bad/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "[INFO|datasets.py:162] 02/01/2021 22:11:31 >> Creating features from dataset file at data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-bad\n",
      "[INFO|processors.py:119] 02/01/2021 22:11:33 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:11:33 >> guid: test-412232\n",
      "[INFO|processors.py:121] 02/01/2021 22:11:33 >> features: InputFeatures(input_ids=[101, 2057, 2985, 1016, 6385, 2045, 1010, 6581, 2282, 2326, 1004, 2393, 2012, 1996, 2392, 4624, 1012, 2035, 3095, 2001, 5379, 1012, 1045, 2097, 2994, 2045, 2153, 999, 102, 8013, 2326, 1027, 1015, 2732, 1045, 17414, 2023, 3309, 2083, 10036, 26348, 8454, 1012, 4012, 1998, 3825, 4469, 3784, 2005, 1996, 3035, 2793, 1998, 3469, 2282, 1012, 2065, 2151, 2028, 2038, 17414, 14555, 2083, 2122, 11744, 1010, 2070, 3749, 2000, 12200, 2005, 1037, 7408, 1012, 4312, 2015, 1010, 1045, 2031, 2589, 2023, 2116, 2335, 2077, 1998, 2296, 3309, 2038, 8686, 2009, 1012, 2061, 2017, 2113, 2073, 1045, 1005, 1049, 2183, 2007, 2023, 1012, 1012, 1012, 2023, 3309, 1005, 1055, 8013, 2326, 2003, 2485, 2000, 9152, 2140, 1012, 2043, 1045, 2288, 2045, 2000, 4638, 1011, 1999, 1010, 1045, 2001, 2445, 1037, 2282, 2007, 1037, 3313, 2793, 1998, 2001, 2409, 2008, 2027, 2020, 2853, 2041, 3294, 2005, 1996, 5353, 1012, 15798, 1010, 1045, 6267, 2041, 2026, 2636, 2005, 1996, 3462, 1998, 3309, 2247, 2007, 1996, 7909, 2005, 1996, 12200, 1012, 1996, 3203, 2551, 2045, 2921, 4129, 2033, 2008, 2027, 2071, 2025, 2079, 2505, 2055, 1996, 2282, 2138, 1045, 3825, 1037, 2353, 2194, 1998, 2027, 2018, 2498, 2000, 2079, 2007, 2008, 1012, 2016, 2921, 4129, 2033, 2008, 2027, 2020, 2853, 2041, 1998, 1045, 2071, 2025, 2022, 2445, 2054, 1045, 2018, 3825, 2005, 1012, 14205, 2008, 1045, 2572, 1010, 1045, 2768, 2045, 1998, 5275, 2007, 2014, 2008, 1045, 2572, 2025, 3625, 2005, 2037, 28616, 9006, 23041, 21261, 2007, 1996, 4037, 2073, 1045, 4149, 2026, 7281, 2013, 1012, 2044, 9177, 2005, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:11:33 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:11:33 >> guid: test-65259\n",
      "[INFO|processors.py:121] 02/01/2021 22:11:33 >> features: InputFeatures(input_ids=[101, 2045, 2003, 2242, 2055, 1996, 3028, 2008, 2038, 1037, 2126, 1997, 5155, 6429, 28496, 3111, 2008, 2298, 2066, 4920, 1999, 1996, 3681, 1012, 2061, 2059, 2017, 3298, 2627, 2068, 2154, 1999, 1998, 2154, 2041, 1998, 2196, 2428, 2228, 3807, 1012, 1996, 2028, 2154, 1010, 2017, 2633, 2175, 1998, 2228, 1010, 1000, 2339, 1010, 2339, 1010, 2339, 1010, 2134, 1005, 1056, 1045, 2175, 10076, 1029, 1000, 2156, 1010, 2008, 3047, 2000, 2033, 1012, 1045, 2572, 1037, 4121, 7632, 7693, 2072, 5470, 1998, 2096, 1045, 3046, 2025, 2000, 4521, 1999, 2062, 2084, 1037, 3232, 2335, 1037, 2095, 2045, 2003, 2242, 2055, 2009, 2008, 1005, 1055, 2074, 2061, 1012, 4365, 1012, 2204, 1012, 2061, 1010, 27178, 7088, 2038, 2026, 3789, 2058, 2001, 28518, 2030, 2151, 2060, 2334, 3182, 1045, 2031, 2042, 1012, 2009, 1005, 1055, 21125, 2625, 1998, 1996, 2833, 2003, 3492, 2714, 1012, 1045, 2036, 2066, 2008, 27178, 7088, 2987, 1005, 1056, 2079, 2035, 1996, 15703, 8288, 6886, 1998, 5003, 4328, 7491, 2096, 2017, 1005, 2128, 2667, 2000, 5959, 2115, 4596, 1012, 2119, 2079, 1996, 2168, 2477, 2833, 7968, 3272, 2012, 2001, 28518, 2017, 2131, 27130, 2021, 2428, 1010, 2079, 1045, 2342, 2062, 2482, 5910, 1029, 2326, 2001, 4248, 1010, 1999, 2755, 1010, 2023, 2173, 2003, 19059, 1012, 2488, 7597, 2205, 1012, 2066, 102, 1996, 2847, 11891, 1998, 1996, 2833, 2003, 19960, 3695, 16748, 2005, 1996, 3976, 1012, 2074, 2055, 2151, 2060, 1000, 3653, 14192, 6651, 12846, 1000, 4825, 10299, 2023, 2173, 1012, 7632, 7693, 2063, 2900, 1010, 2005, 6013, 1010, 2003, 4276, 1996, 2769, 1012, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:11:33 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:11:33 >> guid: test-389217\n",
      "[INFO|processors.py:121] 02/01/2021 22:11:33 >> features: InputFeatures(input_ids=[101, 2204, 2833, 1010, 2200, 18006, 1998, 2012, 6528, 6024, 2326, 1012, 2026, 6904, 2615, 9841, 2003, 1996, 1000, 21734, 1000, 12486, 2460, 10335, 1010, 2460, 10335, 2024, 2941, 11655, 5084, 1999, 1037, 28126, 3993, 2601, 12901, 6229, 2991, 1011, 2125, 1011, 1996, 1011, 5923, 8616, 1010, 7078, 12090, 1012, 12486, 5923, 11350, 2003, 1996, 2160, 12233, 9841, 1998, 5121, 4276, 1037, 3046, 1010, 5474, 1998, 24881, 2665, 20949, 16143, 2006, 1996, 2795, 2005, 4487, 2100, 2161, 2075, 1012, 1045, 2572, 2893, 7501, 2004, 1045, 4339, 2023, 1012, 1012, 1012, 102, 6898, 2074, 2333, 2000, 2023, 20065, 2061, 2057, 2787, 2000, 2298, 2005, 1037, 4759, 2173, 2000, 4521, 1012, 1045, 1005, 2310, 2042, 26369, 2005, 15960, 5923, 11350, 2005, 8811, 2085, 1998, 2245, 2151, 4759, 4825, 2442, 3710, 15960, 5923, 11350, 2061, 2057, 2074, 2253, 1999, 1012, 2061, 1045, 2246, 2012, 1996, 12183, 1012, 2821, 2053, 999, 2053, 15960, 5923, 11350, 2005, 2033, 1012, 10303, 4759, 9588, 6749, 1996, 12486, 5923, 11350, 2061, 1045, 2253, 2007, 2009, 1012, 6898, 3641, 1996, 4086, 12486, 11350, 1012, 6898, 2134, 1005, 1056, 2066, 2151, 1997, 1996, 2217, 9841, 1998, 1045, 2074, 7351, 2068, 2035, 4661, 1996, 5035, 5428, 8840, 2140, 1996, 11350, 2015, 2234, 3435, 1012, 2978, 9364, 2055, 1996, 5785, 3426, 2009, 1005, 1055, 2025, 6379, 5785, 1012, 2018, 1037, 10668, 1997, 2026, 11350, 1012, 28126, 3238, 1012, 2794, 6197, 1997, 5474, 2000, 2074, 5587, 1037, 2978, 1997, 5510, 1012, 6898, 2018, 1037, 10668, 1997, 2026, 11350, 2056, 2009, 1005, 1055, 2062, 2066, 3644, 2785, 1997, 5510, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:11:33 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:11:33 >> guid: test-40556\n",
      "[INFO|processors.py:121] 02/01/2021 22:11:33 >> features: InputFeatures(input_ids=[101, 4342, 2055, 2023, 2173, 2013, 6300, 14277, 4391, 1998, 2106, 2025, 4487, 3736, 9397, 25785, 1012, 8607, 4328, 2721, 1006, 23689, 2050, 1007, 3954, 1998, 25180, 3367, 8303, 2003, 3716, 3085, 1010, 2658, 2007, 2204, 20874, 1006, 2005, 6708, 4781, 1007, 1012, 2253, 2045, 2000, 2131, 2026, 5727, 20877, 4377, 21727, 1998, 8776, 1012, 2165, 2055, 1037, 2733, 1012, 3463, 2020, 13352, 2140, 1012, 2014, 4377, 4906, 3599, 2000, 1996, 15480, 1013, 11702, 2057, 4900, 1012, 3811, 16755, 1012, 2009, 2001, 4276, 1996, 3298, 2013, 6289, 24281, 15851, 2063, 1012, 102, 1045, 2081, 1037, 2655, 2000, 14407, 3054, 1011, 2233, 2074, 2066, 1045, 2079, 2296, 2095, 2005, 2026, 2684, 2203, 1997, 2095, 5337, 16705, 1012, 1045, 2001, 4527, 2000, 4553, 2008, 2016, 2018, 2853, 2014, 4497, 1998, 2333, 1012, 1045, 6618, 2014, 6110, 2052, 2022, 2986, 2144, 14407, 2018, 2589, 2107, 2327, 18624, 2147, 1045, 2018, 2053, 4797, 2008, 1996, 2047, 3954, 2052, 2022, 2074, 2004, 2204, 1012, 6854, 2008, 2001, 2025, 2026, 3325, 1012, 1045, 2001, 3322, 9339, 1002, 8574, 2021, 2043, 1045, 2363, 2014, 2655, 2005, 15373, 2016, 2018, 2992, 1996, 3976, 2019, 3176, 1002, 2871, 1012, 1045, 4188, 2000, 3477, 2004, 1045, 3530, 2000, 1996, 2275, 3976, 1012, 2016, 2106, 11113, 5178, 2000, 1996, 1002, 8574, 2021, 2001, 2200, 12726, 2076, 1996, 2345, 11414, 1012, 1045, 2572, 2200, 16408, 1997, 2235, 2449, 5608, 2004, 2026, 3129, 1998, 1045, 2743, 2028, 2005, 2570, 2086, 1012, 11504, 2016, 2097, 3191, 2023, 1998, 4553, 2008, 2017, 3685, 7438, 2115, 6304, 2008, 2126, 2065, 2017, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|processors.py:119] 02/01/2021 22:11:33 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:11:33 >> guid: test-414191\n",
      "[INFO|processors.py:121] 02/01/2021 22:11:33 >> features: InputFeatures(input_ids=[101, 1045, 2064, 1005, 1056, 3413, 2039, 2019, 13260, 2000, 6723, 2070, 4916, 2833, 1998, 2023, 2173, 2106, 2025, 4487, 3736, 9397, 25785, 1012, 1996, 25545, 3475, 1005, 1056, 2505, 2000, 4339, 2188, 2055, 2021, 2008, 1005, 1055, 2986, 2007, 2033, 1012, 2165, 3602, 2008, 1996, 3696, 2006, 1996, 2341, 2758, 2011, 16429, 1012, 2057, 2020, 3202, 11188, 2011, 1037, 5379, 8241, 1012, 2002, 4384, 2008, 2057, 2018, 2019, 10527, 2007, 2149, 1998, 4046, 2149, 1037, 2152, 3242, 1012, 1045, 2716, 2026, 27427, 8586, 17417, 3726, 2388, 2247, 2040, 2001, 2635, 2014, 4086, 2051, 3752, 1996, 12183, 1012, 2002, 2001, 2200, 5776, 2007, 2149, 1998, 3024, 3435, 1998, 5379, 2326, 1012, 2096, 22331, 1998, 9107, 2070, 11772, 1998, 26509, 1045, 4384, 2129, 18066, 1996, 4044, 2003, 1012, 1996, 102, 1045, 8823, 2182, 2197, 2305, 2007, 2026, 2155, 1012, 1045, 2018, 1037, 16521, 2029, 2001, 2235, 2005, 1996, 3465, 1012, 12595, 3100, 1012, 3071, 2842, 3641, 4121, 29236, 2029, 5031, 1997, 2482, 3490, 10230, 1010, 6904, 18902, 3022, 1010, 4372, 5428, 27266, 3022, 1010, 4385, 1012, 1012, 1012, 2027, 3262, 10865, 2055, 1996, 17153, 28345, 3022, 2108, 5186, 16031, 8029, 1999, 14902, 1998, 1996, 26509, 2001, 2200, 24171, 1010, 2066, 2009, 2018, 2908, 2919, 1012, 2651, 2027, 2024, 2035, 5305, 1012, 1045, 2572, 2025, 2469, 2129, 2027, 2024, 2893, 2035, 2122, 2307, 4391, 1012, 1996, 2833, 2001, 19960, 3695, 16748, 2012, 2190, 1998, 7597, 2020, 2055, 2779, 1012, 1996, 9621, 2503, 2001, 6881, 1998, 2790, 22824, 1012, 2045, 2001, 1037, 19424, 6752, 2104, 2028, 1997, 2256, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=0)\n",
      "[INFO|datasets.py:196] 02/01/2021 22:11:35 >> Saving features into cached file data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-bad/cached_test_BertTokenizerFast_256_same-b [took 2.179 s]\n",
      "[INFO|filelock.py:318] 02/01/2021 22:11:35 >> Lock 139997468515536 released on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-bad/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "No 'DISCORD_TOKEN' or 'DISCORD_CHANNEL' set! Disable Discord notifier.\n",
      "[INFO|trainer.py:361] 02/01/2021 22:11:37 >> *** Evaluate 'test' ***\n",
      "[INFO|trainer.py:1333] 2021-02-01 22:11:37,092 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1334] 2021-02-01 22:11:37,093 >>   Num examples = 17704\n",
      "[INFO|trainer.py:1335] 2021-02-01 22:11:37,093 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████| 1107/1107 [01:40<00:00, 10.91it/s]/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:4196: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|integrations.py:513] 2021-02-01 22:13:18,709 >> Trainer is attempting to log a value of \"{'not same': {'precision': 1.0, 'recall': 0.8582806145503841, 'f1-score': 0.9237362837776224, 'support': 17704}, 'same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'accuracy': 0.8582806145503841, 'macro avg': {'precision': 0.5, 'recall': 0.42914030727519203, 'f1-score': 0.4618681418888112, 'support': 17704}, 'weighted avg': {'precision': 1.0, 'recall': 0.8582806145503841, 'f1-score': 0.9237362837776224, 'support': 17704}}\" of type <class 'dict'> for key \"eval_class_report\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "[WARNING|integrations.py:301] 2021-02-01 22:13:18,711 >> Trainer is attempting to log a value of \"{'not same': {'precision': 1.0, 'recall': 0.8582806145503841, 'f1-score': 0.9237362837776224, 'support': 17704}, 'same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'accuracy': 0.8582806145503841, 'macro avg': {'precision': 0.5, 'recall': 0.42914030727519203, 'f1-score': 0.4618681418888112, 'support': 17704}, 'weighted avg': {'precision': 1.0, 'recall': 0.8582806145503841, 'f1-score': 0.9237362837776224, 'support': 17704}}\" of type <class 'dict'> for key \"eval/class_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|███████████████████████████████████████| 1107/1107 [01:41<00:00, 10.91it/s]\n",
      "[INFO|trainer.py:393] 02/01/2021 22:13:18 >> ***** Eval 'test' results same-b *****\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_loss = 0.3246125280857086\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_acc = 0.8582806145503841\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_f1 = 0.0\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_acc_and_f1 = 0.42914030727519203\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_pearson = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_spearmanr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_corr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:13:18 >>   eval_class_report = {'accuracy': 0.8582806145503841,\n",
      " 'macro avg': {'f1-score': 0.4618681418888112,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.42914030727519203,\n",
      "               'support': 17704},\n",
      " 'not same': {'f1-score': 0.9237362837776224,\n",
      "              'precision': 1.0,\n",
      "              'recall': 0.8582806145503841,\n",
      "              'support': 17704},\n",
      " 'same': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
      " 'weighted avg': {'f1-score': 0.9237362837776224,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 0.8582806145503841,\n",
      "                  'support': 17704}}\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test:   0%|          | 0/17403 [00:00<?, ?it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Eval good-good: 17403\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "test: 100%|██████████| 17403/17403 [00:00<00:00, 176131.92it/s]\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[WARNING|trainer.py:194] 02/01/2021 22:13:20 >> Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\n",
      "[INFO|trainer.py:204] 02/01/2021 22:13:20 >> Training/evaluation parameters MyTrainingArguments(output_dir='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pairtype/good-good', overwrite_output_dir=False, do_train=False, do_eval=False, do_predict=False, evaluation_strategy=<EvaluationStrategy.NO: 'no'>, prediction_loss_only=False, per_device_train_batch_size=8, per_device_eval_batch_size=16, per_gpu_train_batch_size=None, per_gpu_eval_batch_size=None, gradient_accumulation_steps=1, eval_accumulation_steps=None, learning_rate=5e-05, weight_decay=0.0, adam_beta1=0.9, adam_beta2=0.999, adam_epsilon=1e-08, max_grad_norm=1.0, num_train_epochs=3.0, max_steps=-1, warmup_steps=0, logging_dir='runs/Feb01_22-13-20_cuda2', logging_first_step=False, logging_steps=500, save_steps=500, save_total_limit=None, no_cuda=False, seed=42, fp16=False, fp16_opt_level='O1', local_rank=-1, tpu_num_cores=None, tpu_metrics_debug=False, debug=False, dataloader_drop_last=False, eval_steps=500, dataloader_num_workers=0, past_index=-1, run_name='bert-base-uncased-yelp-pair-b_256_16-acc64_3-good-good', disable_tqdm=False, remove_unused_columns=True, label_names=None, load_best_model_at_end=False, metric_for_best_model=None, greater_is_better=None, do_test=True)\n",
      "[INFO|trainer.py:205] 02/01/2021 22:13:20 >> Dataset parameters SamenessDataTrainingArguments(task_name='same-b', data_dir='data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-good', max_seq_length=256, overwrite_cache=False, pad_to_max_length=True)\n",
      "[INFO|trainer.py:206] 02/01/2021 22:13:20 >> Model parameters ModelArguments(model_name_or_path='./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3', config_name=None, tokenizer_name=None, cache_dir=None, use_fast_tokenizer=True)\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:13:20,544 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:13:20,544 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|configuration_utils.py:409] 2021-02-01 22:13:20,545 >> loading configuration file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/config.json\n",
      "[INFO|configuration_utils.py:447] 2021-02-01 22:13:20,545 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-uncased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"same-b\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:1685] 2021-02-01 22:13:20,545 >> Model name './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:13:20,545 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1721] 2021-02-01 22:13:20,545 >> Didn't find file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/added_tokens.json. We won't load it.\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:13:20,545 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/vocab.txt\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:13:20,545 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:13:20,545 >> loading file None\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:13:20,545 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:1766] 2021-02-01 22:13:20,545 >> loading file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/tokenizer_config.json\n",
      "[INFO|modeling_utils.py:938] 2021-02-01 22:13:20,569 >> loading weights file ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3/pytorch_model.bin\n",
      "[INFO|modeling_utils.py:1056] 2021-02-01 22:13:22,539 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "[INFO|modeling_utils.py:1064] 2021-02-01 22:13:22,539 >> All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./output_sent/bert-base-uncased-yelp-pair-b_256_16-acc64_3.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "[INFO|filelock.py:274] 02/01/2021 22:13:22 >> Lock 140234434346336 acquired on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-good/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "[INFO|datasets.py:162] 02/01/2021 22:13:22 >> Creating features from dataset file at data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-good\n",
      "[INFO|processors.py:119] 02/01/2021 22:13:24 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:13:24 >> guid: test-346285\n",
      "[INFO|processors.py:121] 02/01/2021 22:13:24 >> features: InputFeatures(input_ids=[101, 2026, 4004, 6147, 2428, 9002, 2000, 2026, 2293, 2005, 2035, 2477, 11112, 3022, 1012, 2748, 1010, 1045, 2572, 20505, 2043, 2009, 3310, 2000, 2833, 2138, 1045, 2215, 2000, 4521, 2673, 1012, 5189, 1010, 19633, 1998, 22766, 2003, 1996, 2190, 2126, 2000, 6235, 1996, 7224, 1012, 2021, 4365, 999, 1996, 2833, 2003, 2204, 1998, 1996, 8974, 2024, 2130, 2488, 999, 2009, 1005, 1055, 1037, 2307, 2173, 2000, 3942, 2065, 2017, 1005, 2128, 3347, 26397, 1999, 1996, 2181, 2021, 1045, 2876, 1005, 1056, 2360, 2009, 1005, 1055, 1996, 4012, 14213, 2102, 2173, 2000, 10720, 2035, 2305, 2593, 1012, 1996, 2833, 2003, 11937, 21756, 1998, 1996, 7597, 2024, 2200, 9608, 1012, 2023, 2173, 4240, 2009, 1005, 1055, 3800, 2092, 1012, 1037, 2173, 2000, 6723, 1037, 4248, 6805, 1998, 1037, 102, 1045, 2293, 2023, 2173, 999, 999, 12486, 16985, 7559, 2063, 1010, 2192, 1011, 4565, 24799, 1010, 1998, 1042, 10448, 2063, 24665, 3022, 2024, 2000, 3280, 2005, 1012, 999, 999, 1045, 2036, 2293, 1996, 2755, 2008, 2027, 2113, 2129, 2000, 3710, 2035, 4438, 18901, 2015, 2182, 1012, 1045, 2467, 2031, 1996, 2197, 2773, 1012, 1012, 1012, 3972, 4509, 999, 2572, 15599, 3401, 2003, 4658, 1010, 2189, 2003, 1037, 2978, 2205, 5189, 1010, 1998, 2326, 1012, 1012, 1012, 2821, 2023, 2064, 2022, 1037, 3291, 1012, 1017, 2041, 1997, 1019, 2335, 1045, 2272, 2182, 1010, 1045, 2097, 2681, 10206, 2007, 1996, 2326, 1012, 1045, 2293, 1996, 3095, 1010, 2021, 2027, 2071, 5791, 2022, 1037, 2210, 2062, 2012, 6528, 6024, 1012, 2066, 1010, 2009, 1005, 1055, 4569, 2000, 2156, 2068, 2383, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:13:24 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:13:24 >> guid: test-1686\n",
      "[INFO|processors.py:121] 02/01/2021 22:13:24 >> features: InputFeatures(input_ids=[101, 2023, 2173, 2038, 1996, 2190, 21209, 1012, 2307, 2111, 1010, 2189, 2003, 2006, 2391, 1010, 1998, 2040, 2064, 9507, 2216, 2980, 2175, 3995, 10487, 1029, 999, 102, 4875, 15479, 2003, 2019, 8052, 2173, 1012, 3659, 2058, 2093, 8158, 2007, 2019, 14269, 2614, 2291, 14107, 2041, 2160, 1998, 21416, 2189, 1010, 1037, 2358, 3217, 4783, 2422, 2265, 1998, 13594, 3775, 2135, 13681, 10487, 2023, 2003, 13366, 14776, 2135, 1037, 2173, 2005, 2151, 2969, 27818, 5637, 3124, 2000, 2156, 1998, 2022, 2464, 1012, 1996, 2341, 3095, 2024, 5379, 1998, 4025, 2000, 2022, 1999, 6664, 1997, 1037, 3168, 1997, 17211, 2029, 2001, 1037, 8242, 10514, 18098, 5562, 2109, 2004, 2057, 2024, 2000, 7743, 2100, 2030, 7505, 2135, 2341, 3095, 2040, 4025, 4340, 2025, 2000, 2292, 3087, 1999, 2000, 2037, 5069, 1012, 1996, 4211, 7408, 2106, 12072, 1037, 2978, 2021, 2053, 2062, 2061, 2084, 5156, 2414, 2252, 7597, 1998, 6195, 1996, 2307, 2305, 2041, 2057, 2018, 1045, 2514, 2009, 2001, 1037, 4189, 3976, 2000, 3477, 1012, 2085, 2000, 1996, 2364, 8432, 1010, 1996, 3239, 9485, 2006, 3749, 2003, 2003, 3239, 25813, 2135, 2980, 1010, 1996, 2723, 1999, 1996, 2364, 2112, 1997, 1996, 2252, 3352, 7078, 27653, 2007, 4906, 24909, 3797, 3238, 2273, 2875, 1996, 3732, 2112, 1997, 1996, 3944, 1012, 1996, 25545, 2003, 3492, 4658, 1998, 10124, 2923, 2007, 7167, 1997, 3384, 1010, 20437, 1998, 18546, 1998, 1996, 21674, 2024, 8242, 1998, 20133, 2084, 2087, 1012, 3347, 7597, 2681, 1037, 2978, 2000, 2022, 9059, 1998, 2045, 2003, 1996, 5156, 4989, 1997, 8974, 2006, 3749, 12456, 6806, 27225, 3904, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:13:24 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:13:24 >> guid: test-410171\n",
      "[INFO|processors.py:121] 02/01/2021 22:13:24 >> features: InputFeatures(input_ids=[101, 2234, 2182, 2005, 1037, 6062, 12403, 2154, 2007, 2026, 6898, 1012, 2057, 2018, 4149, 1037, 7427, 2125, 2177, 2239, 1012, 2177, 2239, 2443, 2382, 8117, 21881, 1010, 3180, 23624, 1998, 21877, 4305, 1012, 2017, 1005, 2128, 2356, 2000, 2202, 2125, 2115, 6007, 2012, 1996, 4211, 1998, 2000, 2404, 2006, 2122, 2477, 2008, 2298, 2066, 14961, 2091, 2606, 16996, 2017, 1005, 1040, 2424, 1999, 1996, 2902, 1012, 1996, 3095, 2012, 1996, 2392, 2020, 5379, 1998, 7039, 2149, 1999, 2855, 1012, 2574, 1996, 3057, 25069, 2000, 2149, 2234, 1998, 2288, 2149, 1012, 1996, 21881, 2015, 2020, 1999, 3584, 4734, 1012, 2009, 1005, 1055, 1037, 3835, 2572, 15599, 3401, 1012, 2027, 2134, 1005, 1056, 2031, 2151, 25738, 14006, 2030, 2505, 2066, 2008, 1998, 2027, 2209, 2115, 5171, 19613, 12403, 2189, 1012, 1996, 21881, 2993, 2003, 1037, 23370, 21881, 2021, 9826, 1010, 2005, 2033, 2009, 2347, 1005, 1056, 2205, 19613, 1012, 2009, 2347, 1005, 1056, 1996, 3095, 1005, 1055, 6346, 2295, 1012, 1045, 1005, 1049, 2074, 3565, 16356, 13602, 1998, 2026, 2067, 2003, 2788, 102, 2307, 3325, 999, 2253, 1999, 2005, 1037, 5806, 6305, 23624, 23887, 6098, 2008, 2001, 3733, 2000, 2338, 1998, 3095, 2001, 3565, 5379, 1012, 15935, 1010, 2040, 2106, 2026, 10063, 1010, 2001, 4010, 1010, 16222, 19506, 16616, 1998, 2012, 6528, 6024, 2000, 2026, 3791, 1012, 1996, 2392, 4624, 3095, 2001, 8114, 1998, 2323, 2022, 7098, 1997, 3095, 2040, 2024, 5525, 10326, 2000, 3073, 1996, 2190, 8013, 2326, 2000, 7846, 1012, 3893, 3325, 3452, 1998, 1045, 1005, 1049, 2559, 2830, 2000, 2026, 2279, 6098, 999, 102], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:13:24 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:13:24 >> guid: test-70749\n",
      "[INFO|processors.py:121] 02/01/2021 22:13:24 >> features: InputFeatures(input_ids=[101, 10140, 25545, 999, 1999, 1996, 3044, 1005, 1055, 8232, 1999, 2670, 4048, 5603, 8653, 999, 999, 2057, 2036, 2018, 1996, 6548, 2098, 6763, 1998, 2027, 2020, 2428, 2204, 1996, 27467, 2098, 14855, 2721, 11837, 2080, 2347, 1005, 1056, 2205, 25482, 1010, 2074, 1037, 2210, 5926, 2012, 1996, 2203, 999, 999, 2005, 2023, 2210, 3907, 2611, 2001, 3819, 999, 999, 102, 12382, 8810, 1997, 11937, 21756, 2833, 999, 3829, 3626, 2001, 5791, 2551, 2524, 1012, 3095, 2001, 2012, 6528, 6024, 2004, 2092, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "[INFO|processors.py:119] 02/01/2021 22:13:24 >> *** Example ***\n",
      "[INFO|processors.py:120] 02/01/2021 22:13:24 >> guid: test-123644\n",
      "[INFO|processors.py:121] 02/01/2021 22:13:24 >> features: InputFeatures(input_ids=[101, 2326, 2001, 4248, 1010, 3095, 2001, 5379, 2833, 2003, 11519, 2021, 2025, 12476, 1012, 7597, 2024, 4189, 1010, 1045, 1005, 2310, 2018, 2488, 11957, 5572, 2021, 2009, 1005, 1055, 1996, 2069, 2173, 1999, 18176, 2008, 2038, 4248, 3710, 11957, 5572, 1012, 1012, 1012, 2061, 2027, 17704, 2031, 1996, 3006, 25878, 2006, 2023, 2028, 1012, 2065, 2017, 2175, 1999, 6293, 2000, 1996, 9101, 4989, 1997, 2833, 2673, 2842, 2074, 3475, 1005, 1056, 2008, 2204, 1012, 102, 2009, 1005, 1055, 2061, 3835, 2000, 2031, 1037, 2204, 11957, 5572, 2173, 1999, 18176, 999, 2234, 2013, 26197, 2073, 2296, 3420, 2018, 11957, 5572, 1010, 2061, 2009, 2001, 2307, 2000, 2031, 1037, 2173, 2061, 2485, 1012, 2054, 1005, 1055, 2036, 3835, 2055, 2023, 2173, 2003, 2027, 3710, 4248, 12278, 1998, 2036, 2822, 1010, 9101, 1998, 7273, 2202, 5833, 1012, 4438, 10447, 2008, 2020, 2081, 4840, 1998, 2980, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], attention_mask=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], token_type_ids=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], label=1)\n",
      "[INFO|datasets.py:196] 02/01/2021 22:13:26 >> Saving features into cached file data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-good/cached_test_BertTokenizerFast_256_same-b [took 2.184 s]\n",
      "[INFO|filelock.py:318] 02/01/2021 22:13:26 >> Lock 140234434346336 released on data/sentiment/bert-base-uncased-yelp-pair-b_256_16-acc64_3/good-good/cached_test_BertTokenizerFast_256_same-b.lock\n",
      "No 'DISCORD_TOKEN' or 'DISCORD_CHANNEL' set! Disable Discord notifier.\n",
      "[INFO|trainer.py:361] 02/01/2021 22:13:28 >> *** Evaluate 'test' ***\n",
      "[INFO|trainer.py:1333] 2021-02-01 22:13:28,054 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:1334] 2021-02-01 22:13:28,054 >>   Num examples = 17403\n",
      "[INFO|trainer.py:1335] 2021-02-01 22:13:28,054 >>   Batch size = 16\n",
      "100%|███████████████████████████████████████| 1088/1088 [01:39<00:00, 10.93it/s]/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:3845: PearsonRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(PearsonRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/scipy/stats/stats.py:4196: SpearmanRConstantInputWarning: An input array is constant; the correlation coefficent is not defined.\n",
      "  warnings.warn(SpearmanRConstantInputWarning())\n",
      "/disk1/users/ekoerner/miniconda3/envs/sameside/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[WARNING|integrations.py:513] 2021-02-01 22:15:07,983 >> Trainer is attempting to log a value of \"{'not same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'same': {'precision': 1.0, 'recall': 0.8800781474458427, 'f1-score': 0.9362144319814175, 'support': 17403}, 'accuracy': 0.8800781474458427, 'macro avg': {'precision': 0.5, 'recall': 0.44003907372292134, 'f1-score': 0.46810721599070876, 'support': 17403}, 'weighted avg': {'precision': 1.0, 'recall': 0.8800781474458427, 'f1-score': 0.9362144319814175, 'support': 17403}}\" of type <class 'dict'> for key \"eval_class_report\" as a metric. MLflow's log_metric() only accepts float and int types so we dropped this attribute.\n",
      "[WARNING|integrations.py:301] 2021-02-01 22:15:07,985 >> Trainer is attempting to log a value of \"{'not same': {'precision': 0.0, 'recall': 0.0, 'f1-score': 0.0, 'support': 0}, 'same': {'precision': 1.0, 'recall': 0.8800781474458427, 'f1-score': 0.9362144319814175, 'support': 17403}, 'accuracy': 0.8800781474458427, 'macro avg': {'precision': 0.5, 'recall': 0.44003907372292134, 'f1-score': 0.46810721599070876, 'support': 17403}, 'weighted avg': {'precision': 1.0, 'recall': 0.8800781474458427, 'f1-score': 0.9362144319814175, 'support': 17403}}\" of type <class 'dict'> for key \"eval/class_report\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "100%|███████████████████████████████████████| 1088/1088 [01:39<00:00, 10.90it/s]\n",
      "[INFO|trainer.py:393] 02/01/2021 22:15:07 >> ***** Eval 'test' results same-b *****\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_loss = 0.2920215427875519\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_acc = 0.8800781474458427\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_f1 = 0.9362144319814175\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_acc_and_f1 = 0.9081462897136301\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_pearson = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_spearmanr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_corr = nan\n",
      "[INFO|trainer.py:397] 02/01/2021 22:15:07 >>   eval_class_report = {'accuracy': 0.8800781474458427,\n",
      " 'macro avg': {'f1-score': 0.46810721599070876,\n",
      "               'precision': 0.5,\n",
      "               'recall': 0.44003907372292134,\n",
      "               'support': 17403},\n",
      " 'not same': {'f1-score': 0.0, 'precision': 0.0, 'recall': 0.0, 'support': 0},\n",
      " 'same': {'f1-score': 0.9362144319814175,\n",
      "          'precision': 1.0,\n",
      "          'recall': 0.8800781474458427,\n",
      "          'support': 17403},\n",
      " 'weighted avg': {'f1-score': 0.9362144319814175,\n",
      "                  'precision': 1.0,\n",
      "                  'recall': 0.8800781474458427,\n",
      "                  'support': 17403}}\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}